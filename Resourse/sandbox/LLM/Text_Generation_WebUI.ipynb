{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFQl6-FjSYtY"
      },
      "source": [
        "# **Text Generation WebUI**\n",
        "\n",
        "This is a Google Colab notebook for running Oobabooga's [Text Generation WebUI](https://github.com/oobabooga/text-generation-webui), a [Gradio](https://www.gradio.app/)-powered **User Interface** (UI) that can be used to run various **Large Language Models** (LLMs). For more information, [read the TextGen WebUI docs](https://github.com/oobabooga/text-generation-webui/tree/main/docs#readme).\n",
        "\n",
        "----\n",
        "\n",
        "**[Click here for a simple version of this notebook](https://colab.research.google.com/drive/1ztRHfwON9zCeaEiaKPWXIfCDmSYwfzu_).**\n",
        "\n",
        "----\n",
        "\n",
        "<font color=\"pink\">**To run this notebook, scroll down and configure the settings you want, then click on the play button on the left side of the cell. Then wait a few minutes for the links to appear at the bottom.**\n",
        "\n",
        "----\n",
        "\n",
        "### üñ•Ô∏è Models\n",
        "There are a massive variety of Language Models to choose from. Visit [TheBloke's HuggingFace page](https://huggingface.co/TheBloke), the [LocalLLaMA Wiki](https://www.reddit.com/r/LocalLLaMA/wiki/models), and the [LLM Model List](https://github.com/underlines/awesome-marketing-datascience/blob/master/llm-model-list.md) for models and other information.\n",
        "\n",
        "* Performance rankings of models can be found [here](https://github.com/underlines/awesome-marketing-datascience/blob/master/llm-tools.md#benchmarking).\n",
        "\n",
        "#### üö´ Colab limitations\n",
        "\n",
        "Larger models (13B+) and full-precision models (not 8bit/4bit) require heavier GPU resources, which means loading and response times will be slower, **and Colab will likely disconnect you sooner**. If you get disconnected for hitting your free GPU limit, just be patient and come back in a few hours. *Do not try to circumvent these limits or you risk getting banned from Colab.*\n",
        "\n",
        "Most models at 30B and above will not run in Colab at all unless they are quantized to reduce their size first. Even then, don't expect a blazing-fast response time.\n",
        "\n",
        "**About 4K / 8K context models:**\n",
        "  * Larger contexts requires more computing power. **Colab may not have enough resources for running models in full 4K or 8K context**, so if you encounter a CUDA or VRAM error, lower the context size to 2048.\n",
        "\n",
        "----\n",
        "### ü§ñ Characters\n",
        "You can use the following websites to create and share characters compatible with this web UI:\n",
        "\n",
        "* **[Character Card Creator](https://avakson.github.io/character-editor/)** ‚Äî Most interfaces use character cards, which are images that have extra data inside them that define the character.\n",
        "\n",
        "* **[Character Hub](https://chub.ai)** ‚Äî A place to share and download character cards (the site is SFW by default but has a NSFW toggle)\n",
        "\n",
        "* **[PygmalionAI Discord](https://discord.gg/pygmalionai)**\n",
        "\n",
        "----\n",
        "\n",
        "### üèÜ Credit\n",
        "All credit for this notebook goes to the hard work of these people who are much, much smarter than I am.\n",
        "\n",
        "* [Oobabooga's original colab](https://colab.research.google.com/github/oobabooga/AI-Notebooks/blob/main/Colab-TextGen-GPU.ipynb) - by [/u/oobabooga4](https://old.reddit.com/u/oobabooga4)\n",
        "* [ImBlank's ultimate colab](https://colab.research.google.com/drive/18L3akiVE8Y6KKjd8TdPlvadTsQAqXh73) - by [/u/Imblank2](https://old.reddit.com/u/Imblank2)\n",
        "* [FHSenpai's one-click colab](https://colab.research.google.com/drive/1glB99Snng4JmxKiFjTisM0lFPYS5XvvZ?usp=sharing) - by [/u/FHSenpai](https://old.reddit.com/u/FHSenpai)\n",
        "* [ManuDash5's superHOT colab](https://colab.research.google.com/github/ManuDash5/Textgen_webui_NEW/blob/main/TEXT_GEN_WEBUI_8K.ipynb) - by [/u/ManuDashOficial](https://old.reddit.com/u/ManuDashOficial)\n",
        "\n",
        "----\n",
        "\n",
        "### About SillyTavern\n",
        "\n",
        "[SillyTavern](https://github.com/SillyTavern/SillyTavern) is an optional feature-rich interface you can use to interact with Ooba, instead of Gradio. For more information, read the [SillyTavern docs](https://docs.sillytavern.app/).\n",
        "\n",
        "Installing ST costs nothing, it takes about 5 minutes (the same time you spend waiting for this Colab to finish loading), and it takes less than half a GB of disk space (without extensions). **Even absolute potato PCs can run SillyTavern locally.**\n",
        "\n",
        "Try it out for yourself:\n",
        "\n",
        "* [Installation Guide for Windows](https://docs.sillytavern.app/installation/windows/)\n",
        "\n",
        "* [Installation Guide for Linux/MacOS](https://docs.sillytavern.app/installation/linuxmacos/)\n",
        "\n",
        "* [Installation Guide for Android](https://docs.sillytavern.app/installation/android-(termux)/)\n",
        "\n",
        "* For iOS users, there is no native ST support (yet), but if you have a PC, you can host ST there, and then access it from your iPhone's web browser - [see this guide to remote hosting](https://docs.sillytavern.app/usage/remoteconnections/).\n",
        "\n",
        "####Installing SillyTavern Extras\n",
        "\n",
        "[Refer to this guide.](https://docs.sillytavern.app/extras/installation/#installation-methods)\n",
        "\n",
        "If you have any questions, feel free to ask the helpful people at [/r/SillyTavernAI](https://www.reddit.com/r/SillyTavernAI/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7TVVj_z4flw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        },
        "outputId": "62b0e7eb-c402-4c32-c7a7-867b1d79a162"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title üéµ Run Silent Audio Player { display-mode: \"form\" }\n",
        "\n",
        "#@markdown üëá Press play on the audio player that appears below. This will keep the Colab tab alive and prevent Google from disconnecting you for inactivity.\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6oyrr4X0wc2",
        "cellView": "form",
        "outputId": "70bc5fc5-d4ac-4c9e-eb77-89bd02beae2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'text-generation-webui'...\n",
            "remote: Enumerating objects: 12857, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 12857 (delta 55), reused 22 (delta 15), pack-reused 12785\u001b[K\n",
            "Receiving objects: 100% (12857/12857), 23.95 MiB | 33.78 MiB/s, done.\n",
            "Resolving deltas: 100% (8777/8777), done.\n",
            "/content/text-generation-webui\n",
            "Ignoring exllamav2: markers 'platform_system != \"Darwin\" and platform_machine != \"x86_64\"' don't match your environment\n",
            "Collecting git+https://github.com/huggingface/transformers@211f93aab95d1c683494e61c3cf8ff10e1f5d6b7 (from -r requirements.txt (line 27))\n",
            "  Cloning https://github.com/huggingface/transformers (to revision 211f93aab95d1c683494e61c3cf8ff10e1f5d6b7) to /tmp/pip-req-build-a8lfbm21\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-a8lfbm21\n",
            "  Running command git rev-parse -q --verify 'sha^211f93aab95d1c683494e61c3cf8ff10e1f5d6b7'\n",
            "  Running command git fetch -q https://github.com/huggingface/transformers 211f93aab95d1c683494e61c3cf8ff10e1f5d6b7\n",
            "  Running command git checkout -q 211f93aab95d1c683494e61c3cf8ff10e1f5d6b7\n",
            "  Resolved https://github.com/huggingface/transformers to commit 211f93aab95d1c683494e61c3cf8ff10e1f5d6b7\n",
            "  Installing build dependencies: started\n"
          ]
        }
      ],
      "source": [
        "#@title ##**üöÄ Start TextGen WebUI**\n",
        "\n",
        "import sys, os, sys, base64, subprocess, json, shutil, requests, time, pathlib, multiprocessing\n",
        "from IPython.display import clear_output, display, HTML\n",
        "from IPython.utils import capture\n",
        "from google.colab import files, drive\n",
        "from PIL import Image\n",
        "\n",
        "#PARAMS\n",
        "#@markdown üëà Configure the settings below, then press this button to start the installation process. The links and any further instructions will appear at the bottom after a few minutes.\n",
        "\n",
        "#@markdown ----\n",
        "#@markdown ####**ü§ó Download Model**\n",
        "\n",
        "#@markdown You have two options for downloading models:\n",
        "#@markdown * **OPTION 1**: Enter any [HuggingFace model repo](https://huggingface.co/models) below in `<Organization>/<model>` format. You can also add `:<branch>` to the end of the name to download a specific branch. A model is provided by default. Click the small arrow on the right side of the input field to view more models.\n",
        "model_repo_download = \"TheBloke/Mythalion-13B-GPTQ\" #@param [\"TheBloke/MythoMax-L2-13B-GPTQ\", \"TheBloke/Mythalion-13B-GPTQ\", \"TheBloke/Huginn-13B-v4.5-GPTQ\", \"TheBloke/Llama-2-13B-GPTQ\", \"TheBloke/Llama-2-13B-chat-GPTQ\", \"TheBloke/CodeLlama-13B-GPTQ\", \"TheBloke/CodeLlama-13B-Python-GPTQ\", \"TheBloke/CodeLlama-13B-Instruct-GPTQ\", \"TheBloke/WizardCoder-Python-13B-V1.0-GPTQ\", \"TheBloke/WizardMath-13B-V1.0-GPTQ\", \"TheBloke/Spring-Dragon-GPTQ\", \"Blackroot/FrankensteinsMonster-13B-GPTQ\", \"TheBloke/MythoMax-L2-Kimiko-v2-13B-GPTQ\", \"TheBloke/MLewdBoros-L2-13B-GPTQ\", \"TheBloke/Asclepius-13B-GPTQ\", \"Blackroot/Hermes-Kimiko-13B-gptq\", \"TheBloke/UndiMix-v2-13B-GPTQ\", \"TheBloke/Luban-13B-GPTQ\", \"TheBloke/LoKuS-13B-GPTQ\", \"TheBloke/Speechless-Llama2-13B-GPTQ\", \"TheBloke/Speechless-Llama2-Hermes-Orca-Platypus-WizardLM-13B-GPTQ\", \"TheBloke/Chronos-Beluga-v2-13B-GPTQ\", \"TheBloke/ReMM-SLERP-L2-13B-GPTQ\", \"TheBloke/ReMM-v2-L2-13B-GPTQ\", \"TheBloke/Airochronos-L2-13B-GPTQ\", \"TheBloke/Nous-Hermes-Llama2-GPTQ\", \"TheBloke/llama-2-13B-Guanaco-QLoRA-GPTQ\", \"TheBloke/Guanaco-3B-Uncensored-v2-GPTQ\", \"TheBloke/MythoLogic-L2-13B-GPTQ\", \"TheBloke/MythoBoros-13B-GPTQ\", \"TheBloke/airoboros-l2-13b-gpt4-m2.0-GPTQ\", \"TheBloke/airoboros-l2-13b-gpt4-2.0-GPTQ\", \"TheBloke/Airoboros-L2-13B-2.1-GPTQ\", \"TheBloke/Spicyboros-13B-2.2-GPTQ\", \"TheBloke/Llama-2-13B-Ensemble-v5-GPTQ\", \"TheBloke/13B-BlueMethod-GPTQ\", \"TheBloke/Kimiko-v2-13B-GPTQ\", \"TheBloke/Carl-Llama-2-13B-GPTQ\", \"TheBloke/Stheno-L2-13B-GPTQ\", \"TheBloke/Stheno-Inverted-L2-13B-GPTQ\", \"TheBloke/Redmond-Puffin-13B-GPTQ\", \"TheBloke/OpenOrca-Platypus2-13B-GPTQ\", \"Austism/chronos-hermes-13b-v2-GPTQ\", \"TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ\", \"TheBloke/WizardLM-1.0-Uncensored-Llama2-13B-GPTQ\", \"TheBloke/vicuna-13B-v1.5-GPTQ\", \"TheBloke/orca_mini_v3_13B-GPTQ\", \"TheBloke/Scarlett-13B-GPTQ\", \"TheBloke/Samantha-1.11-13B-GPTQ\", \"TheBloke/Pygmalion-2-13B-GPTQ\", \"TehVenom/Metharme-13b-4bit-GPTQ\", \"TheBloke/Pygmalion-2-13B-SuperCOT-GPTQ\", \"TheBloke/Nous-Hermes-Llama-2-7B-GPTQ\", \"TheBloke/Llama-2-7B-GPTQ\", \"TheBloke/Llama-2-7b-Chat-GPTQ\", \"TheBloke/Zarablend-L2-7B-GPTQ\", \"TheBloke/llama-2-7B-Guanaco-QLoRA-GPTQ\", \"TheBloke/MythoLogic-Mini-7B-GPTQ\", \"TheBloke/Dolphin-Llama2-7B-GPTQ\", \"TheBloke/Airoboros-L2-7B-2.1-GPTQ\", \"TheBloke/Luna-AI-Llama2-Uncensored-GPTQ\", \"TheBloke/llama2_7b_chat_uncensored-GPTQ\", \"TheBloke/Llama-2-7b-Chat-GPTQ\", \"TheBloke/Kimiko-7B-GPTQ\", \"TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ\", \"TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ\", \"TheBloke/WizardLM-7B-V1.0-Uncensored-GPTQ\", \"TheBloke/Spicyboros-7B-2.2-GPTQ\", \"TheBloke/vicuna-7B-v1.5-GPTQ\", \"TheBloke/orca_mini_v2_7B-GPTQ\", \"TheBloke/LosslessMegaCoder-Llama2-7B-Mini-GPTQ\", \"TheBloke/koala-7B-GPTQ\", \"TheBloke/Scarlett-7B-GPTQ\", \"TheBloke/Samantha-7B-GPTQ\", \"TheBloke/Pygmalion-2-7B-GPTQ\", \"TehVenom/Metharme-7b-4bit-GPTQ-Safetensors\"] {allow-input: true}\n",
        "#@markdown * **OPTION 2**: For GGUF models, you can paste any .gguf file below. (Oobabooga no longer supports GGML.) These use less GPU resources but are generally slower than GPTQ models. Click the small arrow on the right side of the input field to view some examples.\n",
        "single_file_download = \"\" #@param [\"https://huggingface.co/TheBloke/Huginn-13B-v4-GGUF/resolve/main/huginn-13b-v4.Q4_K_M.gguf\", \"https://huggingface.co/TheBloke/Airoboros-L2-13B-2.1-GGUF/resolve/main/airoboros-l2-13b-2.1.Q4_K_M.gguf\"] {allow-input: true}\n",
        "#@markdown If both options are selected, you'll be asked to choose which one to download.\n",
        "\n",
        "#@markdown ----\n",
        "#@markdown ####**‚öôÔ∏è Launcher Settings**\n",
        "launch_arguments = \"\" #@param [\"\"] {allow-input: true}\n",
        "#@markdown > <font color=\"gray\">You can manually type any [command-line flags](https://github.com/oobabooga/text-generation-webui/blob/main/README.md#basic-settings) or [extensions](https://github.com/oobabooga/text-generation-webui/blob/main/docs/Extensions.md) here if you don't want to tick a bunch of boxes below. (Don't do both, or it will break.)\n",
        "save_to_google_drive = \"off\" #@param [\"off\", \"chatlogs and characters\", \"chatlogs, characters, and models\"]\n",
        "#@markdown > <font color=\"gray\">If activated, saves your data to [Google Drive](https://drive.google.com) automatically, so that they will persist across sessions.</font> Remember that chat models are very large, and free Google Drive only provides 15GB.\n",
        "verbose = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=\"gray\">Print character prompts and responses to the console log.\n",
        "multi_user = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=\"gray\">Multi-user mode. Chat histories are not saved or automatically loaded.\n",
        "api = True #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=\"gray\">Get the Oobabooga public API. This is required to run Oobabooga on alternative UIs, such as SillyTavern.\n",
        "\n",
        "#@markdown ----\n",
        "#@markdown ###**‚≠ê Extensions**\n",
        "superbooga = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=\"gray\">Based on [superbig](https://github.com/kaiokendev/superbig) by kaiokendev. An extension that uses ChromaDB to create an arbitrarily large pseudocontext, taking text files, URLs, or pasted text as input.\n",
        "google_translate = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=\"gray\">Activates translation extension, allowing you to communicate with the bot in a different language using [Google Translate](https://translate.google.com).\n",
        "long_replies = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=\"gray\">Forces bot replies to be longer. It works by banning the `\\n` character until a specified minimum number of tokens have been generated, forcing the bot to keep talking for as long as you want.\n",
        "character_bias = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=\"gray\">An extension that adds an user-defined, hidden string at the beginning of the bot's reply with the goal of biasing the rest of the response.\n",
        "silero_tts = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=\"gray\">Text-to-speech extension using [Silero](https://github.com/snakers4/silero-models). When used in chat mode, it replaces the responses with an audio widget. There are 118 voices available (`en_0` to `en_117`), which can be set in the \"Extensions\" tab of the interface. You can find samples here: [Silero samples](https://oobabooga.github.io/silero-samples/).\n",
        "elevenlabs_tts = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=\"gray\">Text-to-speech extension using the [ElevenLabs](https://beta.elevenlabs.io/) API. You need an API key to use it.\n",
        "whisper_stt = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=\"gray\">Speech-to-text extension using [Whisper](https://github.com/openai/whisper). Allows you to enter your inputs in chat mode using your microphone.\n",
        "send_pictures = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=\"gray\">Adds a menu for sending pictures to the bot, which are automatically captioned using [BLIP](https://github.com/salesforce/BLIP).\n",
        "gallery = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=\"gray\">Creates a gallery with the chat characters and their pictures.\n",
        "sd_api_pictures = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=\"gray\">Allows you to request pictures from the bot in chat mode, which will be generated using AUTOMATIC1111's SD API. See examples [here](https://github.com/oobabooga/text-generation-webui/pull/309). Note: You'll need an available instance of AUTOMATIC1111's webui running with an `--api` flag.\n",
        "openai = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=\"gray\">Creates an API that mimics the OpenAI API and can be used as a drop-in replacement.\n",
        "ngrok = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=\"gray\">Allows you to access the web UI remotely using the [ngrok](https://ngrok.com/) reverse tunnel service (free). It's an alternative to the built-in Gradio `--share` feature.\n",
        "\n",
        "#@markdown ----\n",
        "#@markdown ###**‚öôÔ∏è Advanced Settings**\n",
        "settings_file = \" \" #@param [\"https://raw.githubusercontent.com/pcrii/Philo-Colab-Collection/main/settings-colab-template.json\"] {allow-input:true}\n",
        "#@markdown > <font color=\"gray\">Load the default interface settings from a raw text file. Click the arrow to see an example.\n",
        "perplexity_colors = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=\"gray\">Colors each token in the output text by its associated probability, as derived from the model logits.\n",
        "xformers = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=\"gray\">Use [xformers](https://github.com/facebookresearch/xformers)' memory efficient attention. This should increase your tokens/s.\n",
        "deepspeed = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=\"gray\">An alternative way of reducing the GPU memory usage. Enable the use of [DeepSpeed ZeRO-3](https://deepspeed.readthedocs.io/en/latest/zero3.html) for inference via the Transformers integration.\n",
        "auto_devices = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=\"gray\">Automatically split the model across the available GPU and CPU.\n",
        "cpu = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=\"gray\">Use the CPU to generate text. Warning: Extremely slow.\n",
        "no_cache = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=\"gray\">Set `use_cache` to False while generating text. This reduces the VRAM usage a bit with a performance cost.\n",
        "precision = \"default\" #@param [\"default\", \"8bit\", \"4bit\"]\n",
        "#@markdown > <font color=\"gray\">For older models that aren't quantized, you can toggle this setting to load them with 8-bit or 4-bit precision using [bitsandbytes](https://github.com/TimDettmers/bitsandbytes), greatly reducing the GPU memory usage, at a small accuracy cost. You can leave this setting alone if the model you're using has GPTQ, 128, x-bit, or GGML in the name.\n",
        "\n",
        "#@markdown These settings are for older quantized models that have no `quantize_config.json` file. Usually you'll want wbits 4 and groupsize 128.\n",
        "wbits = \"default\" #@param [\"default\", \"2\", \"3\", \"4\", \"8\"]\n",
        "groupsize = \"default\" #@param [\"default\", \"32\", \"64\", \"128\"]\n",
        "\n",
        "\n",
        "trust_remote_code = True\n",
        "share = True\n",
        "run_web_ui = True\n",
        "model = \"\"\n",
        "\n",
        "# Stop program if both model input fields are empty\n",
        "# You WILL download the model, and you will be happy\n",
        "if (model_repo_download == \" \".strip() and single_file_download == \" \".strip()):\n",
        "  print(f\"\\033[92m\\n\\n######################################################\\n\\nNo model selected! Please select a model above, then run this cell again.\\n\\n######################################################\\n\\n\\033[0m\")\n",
        "  sys.exit()\n",
        "\n",
        "#Install ooba\n",
        "def install_ooba():\n",
        "  global launch_arguments\n",
        "  if os.path.exists(repo_dir):\n",
        "    %cd {repo_dir}\n",
        "    !git pull\n",
        "  else:\n",
        "    !git clone https://github.com/oobabooga/text-generation-webui.git\n",
        "  if (\"chatlogs and characters\" in save_to_google_drive):\n",
        "    if not os.path.exists(f\"{base_drive_dir}/oobabooga-data\"):\n",
        "      os.mkdir(f\"{base_drive_dir}/oobabooga-data\")\n",
        "    if not os.path.exists(f\"{base_drive_dir}/oobabooga-data/logs\"):\n",
        "      os.mkdir(f\"{base_drive_dir}/oobabooga-data/logs\")\n",
        "    if not os.path.exists(f\"{base_drive_dir}/oobabooga-data/characters\"):\n",
        "      shutil.move(\"text-generation-webui/characters\", f\"{base_drive_dir}/oobabooga-data/characters\")\n",
        "    else:\n",
        "      !rm -r \"text-generation-webui/characters\"\n",
        "\n",
        "    !ln -s \"$base_drive_dir/oobabooga-data/logs\" \"text-generation-webui/logs\"\n",
        "    !ln -s \"$base_drive_dir/oobabooga-data/characters\" \"text-generation-webui/characters\"\n",
        "  else:\n",
        "    !mkdir text-generation-webui/logs\n",
        "  !ln -s text-generation-webui/logs .\n",
        "  !ln -s text-generation-webui/characters .\n",
        "  !ln -s text-generation-webui/models .\n",
        "  %rm -r sample_data\n",
        "  %cd {repo_dir}\n",
        "  if not (\" \".strip() in settings_file):\n",
        "    !wget {settings_file} -O settings-template.yaml\n",
        "    launch_arguments.add(f'--settings settings-template.yaml')\n",
        "  !pip install -r requirements.txt | grep -v 'already satisfied'\n",
        "  print(f\"\\033[1;32;1m\\nIf you see a warning about packages, just ignore it. There is no need to restart the runtime.\\n\\033[0;37;0m\")\n",
        "  # Install extension req\n",
        "  if (deepspeed) or ('deepspeed' in launch_arguments):\n",
        "    !pip install -U mpi4py | grep -v 'already satisfied'\n",
        "    !pip install -U deepspeed | grep -v 'already satisfied'\n",
        "  if (xformers) or ('xformers' in launch_arguments):\n",
        "    !pip install xformers | grep -v 'already satisfied'\n",
        "  if (api) or ('api' in launch_arguments):\n",
        "    !pip install -r extensions/api/requirements.txt | grep -v 'already satisfied'\n",
        "  if (google_translate) or ('google_translate' in launch_arguments):\n",
        "    !pip install -r extensions/google_translate/requirements.txt | grep -v 'already satisfied'\n",
        "  if (superbooga) or ('superbooga' in launch_arguments):\n",
        "    !pip install -r extensions/superbooga/requirements.txt | grep -v 'already satisfied'\n",
        "  if (silero_tts) or ('silero_tts' in launch_arguments):\n",
        "    !pip install -r extensions/silero_tts/requirements.txt | grep -v 'already satisfied'\n",
        "  if (elevenlabs_tts) or ('elevenlabs_tts' in launch_arguments):\n",
        "    !pip install -r extensions/elevenlabs_tts/requirements.txt | grep -v 'already satisfied'\n",
        "  if (whisper_stt) or ('whisper_stt' in launch_arguments):\n",
        "    !pip install -r extensions/whisper_stt/requirements.txt | grep -v 'already satisfied'\n",
        "  if (openai) or ('openai' in launch_arguments):\n",
        "    !pip install -r extensions/openai/requirements.txt | grep -v 'already satisfied'\n",
        "  if (ngrok) or ('ngrok' in launch_arguments):\n",
        "    !pip install -r extensions/ngrok/requirements.txt | grep -v 'already satisfied'\n",
        "\n",
        "\n",
        "#Mount Google Drive\n",
        "if not (\"off\" in save_to_google_drive):\n",
        "  if (\"chatlogs and characters\" in save_to_google_drive):\n",
        "    drive.mount('/content/drive')\n",
        "    base_drive_dir = \"/content/drive/MyDrive/\"\n",
        "    repo_dir = '/content/text-generation-webui'\n",
        "    %cd /content\n",
        "    install_ooba()\n",
        "  if (\"chatlogs, characters, and models\" in save_to_google_drive):\n",
        "    drive.mount('/content/drive')\n",
        "    base_drive_dir = \"/content/drive/MyDrive/\"\n",
        "    repo_dir = '/content/drive/MyDrive/text-generation-webui'\n",
        "    model_dir = '/content/drive/MyDrive/text-generation-webui/models'\n",
        "    %cd /content/drive/MyDrive\n",
        "    install_ooba()\n",
        "else:\n",
        "  %cd /content\n",
        "  repo_dir = '/content/text-generation-webui'\n",
        "  model_dir = '/content/text-generation-webui/models'\n",
        "  install_ooba()\n",
        "\n",
        "# Repo download\n",
        "def repo_download():\n",
        "  global model\n",
        "  model = model_repo_download\n",
        "  %cd {repo_dir}\n",
        "  !python download-model.py {model}\n",
        "  model = model.replace('/', '_')\n",
        "\n",
        "if (model_repo_download) and not (single_file_download):\n",
        "  clear_output(wait = False)\n",
        "  repo_download()\n",
        "\n",
        "# Single file download\n",
        "def single_download():\n",
        "  global model\n",
        "  def get_filename_from_url(single_file_download):\n",
        "    return os.path.basename(single_file_download)\n",
        "  model = get_filename_from_url(single_file_download)\n",
        "  %cd {model_dir}\n",
        "  !apt install aria2\n",
        "  !aria2c -x 16 -s 16 -o {model} {single_file_download}\n",
        "  %cd {repo_dir}\n",
        "  # GGUF: reinstall llama-cpp-python with BLAS for GPU acceleraton support\n",
        "  !pip uninstall -y llama-cpp-python\n",
        "  !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir\n",
        "\n",
        "if (single_file_download) and not (model_repo_download):\n",
        "  clear_output(wait = False)\n",
        "  single_download()\n",
        "\n",
        "# If both input fields are filled\n",
        "if (model_repo_download and single_file_download):\n",
        "  clear_output(wait = False)\n",
        "  valid_input = False\n",
        "  while not valid_input:\n",
        "    choice = input(\"\\n\\n######################################################\\n\\nYou have selected two models at once. Choose one to download.\\n\\nIn the box below, type 1 for the repo download, or type 2 for the single file download. Then press Enter.\\n\\n\")\n",
        "    choice = int(choice)\n",
        "    if choice == 1:\n",
        "      valid_input = True\n",
        "      repo_download()\n",
        "    elif choice == 2:\n",
        "      valid_input = True\n",
        "      single_download()\n",
        "    else:\n",
        "      print(\"\\nInvalid input.\\n\")\n",
        "\n",
        "# Arguments\n",
        "launch_arguments = set()\n",
        "if ('GPTQ' in model or 'gptq' in model): launch_arguments.add('--loader exllama')\n",
        "if ('.gguf' in model or 'GGUF' in model or 'gguf' in model): launch_arguments.add('--threads 16 --n-gpu-layers 20 --loader ctransformers')\n",
        "if not ('default' in wbits): launch_arguments.add(f'--wbits {wbits}')\n",
        "if not ('default' in groupsize): launch_arguments.add(f'--groupsize {groupsize}')\n",
        "if ('8bit' in precision): launch_arguments.add('--load-in-8bit')\n",
        "if ('4bit' in precision): launch_arguments.add('--load-in-4bit')\n",
        "if trust_remote_code: launch_arguments.add('--trust-remote-code')\n",
        "if multi_user: launch_arguments.add('--multi-user')\n",
        "if verbose: launch_arguments.add('--verbose')\n",
        "if no_cache: launch_arguments.add('--no-cache')\n",
        "if xformers: launch_arguments.add('--xformers')\n",
        "if deepspeed: launch_arguments.add('--deepspeed')\n",
        "if api: launch_arguments.add('--api --public-api')\n",
        "if share: launch_arguments.add('--share')\n",
        "if auto_devices: launch_arguments.add('--auto-devices')\n",
        "if cpu: launch_arguments.add('--cpu')\n",
        "\n",
        "#Extension toggles\n",
        "active_extensions = []\n",
        "if long_replies: active_extensions.append('long_replies')\n",
        "if send_pictures: active_extensions.append('send_pictures')\n",
        "if character_bias: active_extensions.append('character_bias')\n",
        "if google_translate: active_extensions.append('google_translate')\n",
        "if superbooga: active_extensions.append('superbooga')\n",
        "if silero_tts: active_extensions.append('silero_tts')\n",
        "if elevenlabs_tts: active_extensions.append('elevenlabs_tts')\n",
        "if whisper_stt: active_extensions.append('whisper_stt')\n",
        "if gallery: active_extensions.append('gallery')\n",
        "if openai: active_extensions.append('openai')\n",
        "if sd_api_pictures: active_extensions.append('sd_api_pictures')\n",
        "if ngrok: active_extensions.append('ngrok')\n",
        "if perplexity_colors: active_extensions.append('perplexity_colors')\n",
        "\n",
        "# If any extensions are selected:\n",
        "# Append the --extensions flag and all selected extensions\n",
        "if len(active_extensions) > 0:\n",
        "  launch_arguments.add(f'--extensions {\" \".join(active_extensions)}')\n",
        "\n",
        "clear_output(wait = True)\n",
        "\n",
        "# Run WebUI\n",
        "print(f\"\\033[1;32;1m\\n######################################################\\n\\nThe model should load in about a minute. To enter TextGen, click on the link below that ends with gradio.live.\\n\\nFor SillyTavern users, copy the \\\"non-streaming URL\\\" (ends with \\\"/api\\\") and paste it into the \\\"Blocking API URL\\\" in the API settings.\\n\\n######################################################\\n\\033[0;37;0m\")\n",
        "if ('deepspeed' in launch_arguments):\n",
        "  cmd =f\"deepspeed --num_gpus=1 server.py --model {model} {' '.join(launch_arguments)}\"\n",
        "  print(cmd)\n",
        "  !$cmd\n",
        "else:\n",
        "  cmd = f\"python server.py --model {model} {' '.join(launch_arguments)}\"\n",
        "  print(cmd)\n",
        "  !$cmd"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}